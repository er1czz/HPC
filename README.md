# Kaggle data challenge log
### 1. RMS Titanic Survival Prediction (testing water)
- random forest classification, **accuracy** score 0.81930 for training and 0.77033 for testing
- Accuracy score: higher score is better, testing score provided by Kaggle
- to-do: Monte Carlo to simulate the missing data, especially passenger age.
### 2 Ames House Price Prediction (model fitting practice)
- random forest regression, **RMLE** score 0.14319 for training and 0.18125 for testing 
- RMLE (Root Mean Squared Log Error): lower score is better, testing score provided by Kaggle
- to-do: exploratory data analysis and feature selection
### 3 [IEEE-CIS Fraud Detection (training and testing, fullset)](https://github.com/er1czz/kaggle/blob/master/Fraud_Detection_fullset.ipynb)
- data anomaly, skewed data, data preprocess
- Random Forest classifier: reasonable model fitting and anomaly detection 
- XGboost overfitted
- Logistic regression unsuitable

# Misc
### Monte Carlo
- MC calculation on Dice and Pi with Numpy Random

#### Data Sources (Kaggle)  
1. https://www.kaggle.com/c/titanic  
2. https://www.kaggle.com/c/house-prices-advanced-regression-techniques  
3. https://www.kaggle.com/c/ieee-fraud-detection/  
